# Batch_Normalization_ANN
# âš¡ Batch Normalization: Accelerating Training Speed

> **Note:** This README was generated by a custom prompt AI using specific project details provided by the developer to ensure ease of understanding for all readers.

---

## ðŸŽ¯ Project Overview
The goal of this notebook is to demonstrate how **Batch Normalization** can significantly speed up the training process and improve stability in Neural Networks.

### ðŸ“‚ The Dataset
* **Link:** (Available inside the notebook)
* **Type:** Concentric Circle Dataset.
* **Challenge:** This dataset is non-linear and generally difficult for simple models to classify effectively.

<img width="546" height="413" alt="bn1" src="https://github.com/user-attachments/assets/8943fca5-eab8-4101-8fb5-e589188ae0e0" />


---

## âš ï¸ Phase 1: The Baseline Model (Without Batch Norm)

We started by creating a simple Neural Network to establish a baseline.

### Architecture
* **Input Layer**
* **Hidden Layer:** 1 layer with **2 nodes** (ReLU activation).
* **Output Layer:** 1 node (Sigmoid activation).

### Training Results
* **Epochs:** 200
* **Observation:** The model struggled. While accuracy improved, it was **not consistent**. The metrics fluctuated significantly (getting low, then high again) rather than following a smooth curve.

---

## ðŸš€ Phase 2: The Solution (With Batch Normalization)

We improved the model by adding **Batch Normalization** layers.

> **ðŸ“ Important Notebook Note:** Please ensure you reference the file named **`BatchNormalization(1)`**.
> *Warning:* A separate file in this repo contains a minor configuration issue (3 input nodes) which causes errors. `BatchNormalization(1)` is the correct version.

### Architecture Changes
We added Batch Normalization at two specific points:
1.  After the **Input Layer**.
2.  After the **Hidden Layer**.

### Model Summary & Parameters
* Upon compiling, the summary shows **8 non-trainable parameters**.
* These correspond to the **Beta and Gamma (Alpha)** weights for the 4 nodes involved in the normalization layers.

### Training Results
* **Epochs:** 200
* **Observation:** The difference was clear.
    * **Accuracy:** Increased constantly without the wild fluctuations seen in Model 1.
    * **Loss:** Remained consistent and decreased steadily.

---

## ðŸ“Š Comparison & Conclusion

We plotted the history of both models side-by-side to compare performance.

<img width="556" height="413" alt="bn2" src="https://github.com/user-attachments/assets/966a51de-c322-4fca-a812-6b10e787cc43" />


### Key Takeaways
1.  **Convergence:** Model 2 (with Batch Norm) **converged faster** than the baseline.
2.  **Behavior:** While the plot for Model 2 showed some spikes, the overall trend was significantly faster and more reliable than the un-normalized model.
